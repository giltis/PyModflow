# -*- coding: utf-8 -*-
"""
Endpoint and pathline classes for post-processing MODPATH results.

Created on Tue Nov 10 08:57:21 2015
@author: wzell
"""
import math
import pandas as pd
import numpy as np
from datetime import timedelta
import statsmodels.api as sm

import matplotlib.pyplot as plt
plt.ioff()

# --- START MODULE PARAM SET ---

terminated_status = 2   # MP6 status flag for normally-terminated particles

int_to_month={1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}
month_to_int={'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}
quarter_map = {'Jan':'Jan','Feb':'Jan','Mar':'Jan','Apr':'Apr','May':'Apr','Jun':'Apr','Jul':'Jul','Aug':'Jul','Sep':'Jul','Oct':'Oct','Nov':'Oct','Dec':'Oct'}

recharge_to_volume = 1

# --- STOP MODULE PARAM SET ----
def value_from_date(irow,i_ts_object,downscale=None):
    """Finds the date index in a pandas SERIES that is NEAREST to the specified
    date and returns the associated value.i_ts_object can be a series
    (e.g., atm tracer inputs) or a dictionary of arrays (e.g., land surface
    nitrogen inputs).  Note that for nitrogen inputs this may associate a
    recharge date with the following year's estimated inputs.  This approximation
    deemed acceptable given the broader uncertainties associated with the loading
    estimations."""

    idate = irow['RechargeDate']
    irow,icol = irow['Final Row'],irow['Final Column']    
    
    if (type(i_ts_object) is dict):
        # For case in which the passed time series argument is a dictionary:
        # Convert the dictionary keys to a series of dates.
        # The default dictionary has multiple downscaled time
        # series at the second level of the dictionary. Note that this function
        # need to be made smarter in order to accomodate future deviations from
        # this default dict structure
        
        iseries = sorted(i_ts_object[downscale].keys())
        iseries = pd.Series(iseries,iseries)
        
        idx = np.argmin(np.abs(iseries - idate))        
        ival = i_ts_object[downscale][idx][irow,icol]
    
    else:
        # For the case in which the passed time series argument is a pandas Series.
        iseries = i_ts_object    
        idx = np.argmin(np.abs(iseries.index.to_pydatetime() - idate))
        ival = iseries.iloc[idx]

    return ival

def parse_IPCODE(row):
    '''Helper function for translating the MP5 IPCODE
    to MP6 status flags.'''
    
    ip_code = row.IPCODE
    
    if (ip_code < 0):
        id_code = str(int(ip_code))
    else:
        id_code = list(str(int(ip_code)))[-1]

    if (id_code == '-2'): # Unreleased
        istatus = 4
    elif (id_code == '-1'): # Stranded
        istatus = 5
    elif (id_code == '0'): # Still active
        istatus = 1  
    elif (id_code == '1'): # Normally terminated
        istatus = 2
    elif (id_code == '2'): # Zone terminated
        istatus = 3  
    
    return istatus

def mp5_IPCODE(idf):
    '''Determines particle status from the IPCODE. NOTE: uses MP6 status
    convention rather than MP5 IDCODE convention! MP5 IDCODE 1 = MP6 status
    code 2 = discharged normally.'''
        
    idf['Status'] = idf.apply(parse_IPCODE,axis=1)
    idf['Particle ID'] = idf.index
    
    return idf

def read_header(endpoint_file):
    '''Reads the endpoint header, determines whether the endpoint file was
    generated by MODPATH5 or MODPATH6, and counts the number of header lines.'''
    
    with open(endpoint_file,'r') as fin:
        
        all_lines = fin.readlines()
        if 'MODPATH_ENDPOINT_FILE 6' in all_lines[0]:                
            mp5_flag = False
            header_line_count = 1
            for iline in all_lines:
                if 'END HEADER' in iline:
                    break
                else:
                    header_line_count += 1            
            fin.close()
            
        else:
            mp5_flag = True
            header_line_count = 1
            
    return mp5_flag,header_line_count

def parse_endpoint(endpoint_file):
    '''Split the MODPATH endpoint file into two files; normally terminated and still active.
    This splitting may be required in order to accomodate the memory limits encountered when
    reading very large files to a dataframe.'''

    print '\n\nSplitting endpoint file into active and terminated subsets.\n\n'
    
    # Initialize the function parameters   
    active_endpoint_file = endpoint_file + '.active','w'
    terminated_endpoint_file = endpoint_file + '.terminated','w'   
    
    mp5_flag,header_line_count = read_header(endpoint_file)
    
    if (mp5_flag == True):
        status_idx = 17
    else:
        status_idx = 2
        
    with open(endpoint_file,'r') as fin,\
         open(active_endpoint_file) as active_fout,\
         open(terminated_endpoint_file) as terminated_fout:
        
        icount = 0
        for iline in fin:
    
            # Write the header lines           
            while (icount < header_line_count):
                
                active_fout.write(iline)
                terminated_fout.write(iline)
                    
                icount += 1
                continue
            
            istatus = parse_IPCODE(iline.split()[status_idx])            
            
            if (istatus == 1):
                active_fout.write(iline)
                
            if (istatus == 2):
                terminated_fout.write(iline)
                
    return

def write_restart(self,active_df,terminated_df,restart_csv,restart_flag):
    '''Merge the information on restarting particles (derived from a 
    previous endpoint file) with the information on those same particles
    from this endpoint file. NOTE: this assumes that the endpoint file has
    been parsed into terminated and active components.  With modification this
    could also accomodate a full endpoint dataframe that has not yet been split.
    E.g., - 'if terminated_df is not None' . . . and if terminated_df is None,
    assume that the endpoint file has not been split and handle the sorting here.'''

    # Housekeeping for the active_df
    active_df = active_df[['Initial Time','Initial Global X','Initial Global Y','Initial Local Z','Final Column','Final Row','Final Layer','Final Global X','Final Global Y','Final Local Z','ThisTravelTime']]
    
    # If restarting, identify those active particles which were in the system before this model run . . . 
    if (restart_flag == True):
        
        # Load the information from the previous model run        
        restart_df = pd.DataFrame.from_csv(restart_csv)
        restart_df = restart_df.drop(['ThisTravelTime'],axis=1)
                        
        # Read the local coords as strings in order to fix them for use as identifiers
        # in endpoint processing
        for icol in ['Initial Global X','Initial Global Y','Initial Local Z']:
            restart_df[icol] = restart_df[icol].astype(str)
            active_df[icol] = active_df[icol].astype(str)
            terminated_df[icol] = terminated_df[icol].astype(str)

        # Merge the active particles with the restart df in order to associate any prior travel time with each active particle;
        # increment the cumulative time to include all travel time
        active_df = pd.merge(active_df,restart_df,on=(['Initial Global X','Initial Global Y','Initial Local Z']),how='left')
        active_df['CumTravelTime'] = active_df['CumTravelTime'].fillna(0)
        active_df['CumTravelTime'] = active_df['CumTravelTime'] + active_df['ThisTravelTime']
        active_df['Initial Time'] = active_df['Initial Time'] - active_df['CumTravelTime']

        # The modified active_df will be saved as the restart dataframe
        new_restart_df = active_df[['Final Column','Final Row','Final Layer','Final Global X','Final Global Y','Final Local Z','ThisTravelTime','CumTravelTime']]
               
        # Merge the terminated particles with the restart_df
        terminated_df = pd.merge(terminated_df,restart_df,on=['Initial Row','Initial Column','Initial Layer','Initial Global X','Initial Global Y','Initial Local Z'],how='left')
        terminated_df['CumTravelTime'] = terminated_df['CumTravelTime'].fillna(0)
        terminated_df['Initial Time'] = terminated_df['Initial Time'] - terminated_df['CumTravelTime']

    # If not restarting (restart_flag = False), the unmodified active_df will be saved as the restart dataframe    
    else:
        new_restart_df = active_df[['Final Column','Final Row','Final Layer','Final Global X','Final Global Y','Final Local Z','ThisTravelTime']]
        new_restart_df['CumTravelTime'] = new_restart_df['ThisTravelTime']
        
    # Save the new restart df with appropriately re-labeled columns    
    new_restart_df.columns = [x.replace('Final','Initial') for x in new_restart_df.columns]
    new_restart_df.to_csv(restart_csv)
    
    return terminated_df

# ================================
# --- START RECHARGE FUNCTIONS ---
# ================================

def get_recharge_rate(row,irch_ts_dict,idirection):
    '''Helper function that should not be called directly by the user.
    Returns the recharge rate at the water table cell.'''
    
    irates = irch_ts_dict[0]
    
    if (idirection == 'backward'):
        return irates[(row['Final Row']-1),(row['Final Column']-1)] # Conversion to Python indices
        
    if (idirection == 'forward'):
        return irates[(row['Initial Row']-1),(row['Initial Column']-1)] # Conversion to Python indices

def get_recharge_date(row,idirection):
    '''Helper function that should not be called directly by the user.
    For transport observations simulated with backward tracking,
    returns the recharge date given the observation date and the travel time
    to that observation. FORWARD TRACKING NOT YET IMPLEMENTED'''
    
    if (idirection == 'backward'):
        return (row['TobDate'] - timedelta(days=(row['ThisTravelTime'])))
        
    if (idirection == 'forward'):
        return None

def get_rch_volume_per_particle(row,iarea,time_length=1):
    '''Returns the recharge volume assigned to each cell. NEEDS UPDATE 
    (including supporting work elsewhere) in order to derive actual time step
    length. E.g., time_length = recharge_stress_periods[row.PythonTimeStep].''' 
    
    irate = row['RechargeRate']
    iparticles = row['NParticles']    
    ivol = (irate * time_length * iarea)/iparticles
    
    return ivol

def get_timestep(row):
    '''Returns the time step during which the indicated time value is located.
    This is used to identify the recharge amount associated with the particle.
    NOTE: RETURNS TIME STEP IN PYTHON INDEXING (not MODPATH indexing).'''
    
    if (row['Initial Time'] < rch_timesteps[0]):
        istep = 0
    else:
        # Find the maximum time step that is less than the time value
        lower_bounds = rch_timesteps <= row['Initial Time']
        istep = np.max(np.where(lower_bounds == True))
        
    return istep

def map_rch_input(idf,rch_ts_dict,nx=1,ny=1,nz=1,cell_area=None,mpdirection=None):
    '''Associates each particle in a dataframe with a recharge rate (NEED TO CHECK RECHARGE VOLUME).
    Currently assumes steady state simulation; NEEDS METHOD OF ASSOCIATING PARTICLE START TIME WITH
    SELECTION FROM RECHARGE TIME SERIES. rch_ts_dict = recharge time series dictionary [keys=time
    step,values=(nrow,ncol).'''
    
    if (mpdirection == 'backward'):
        # For backward-particle simulations: calculates the recharge date for
        # terminating particles and adds temporal information to the dataframe.'''
        idf['RechargeRate'] = idf.apply(get_recharge_rate,axis=1,args=(rch_ts_dict,'backward'))
        idf['RechargeVol']  = idf['RechargeRate'] * recharge_to_volume                          # <- NOT CORRECT # # #
        
    if (mpdirection == 'forward'):
        # For forward-particle simulations: extract the recharge rate for
        # starting particles from the recharge time series
        idf['NParticles'] = nx * ny * nz            
        idf['RechargeRate'] = idf.apply(get_recharge_rate,axis=1,args=(rch_ts_dict,'forward'))
        idf['RechargeVol']  = idf.apply(get_rch_volume_per_particle,axis=1,args=(cell_area,))
                        
    return idf

# ================================
# --- STOP RECHARGE FUNCTIONS ----
# ================================

def modify_by_list(idf,describe=None,filter_by_col='Particle ID',filter_list=None,modify_col='RechargeConc',remove_fraction=1):
    '''Returns a modified dataframe with a reduced
    solute concentration if the particle is flagged by the remove_by_col argument.
    E.g., for simulation of nitrate removal by denitrification, a list of particle
    ids of those particles which contact the Aquia Confining Unit may be passed,
    together with a removal efficiency of the Confining Unit, in order to
    reduce the total simulated nitrate concentration at the observation
    location.'''    
    
    idf['Before_' + describe] = idf[modify_col].copy()

    # Create the subset of particles which may be removed
    idf['FractionRemaining'] = 1       # Initializes all columns with no removal
    idf.loc[idf[filter_by_col].isin(filter_list),'FractionRemaining'] = 1 - remove_fraction
    idf[modify_col] = idf[modify_col] * idf['FractionRemaining']
    idf = idf.drop('FractionRemaining',axis=1)
    
    return idf
    
def modify_by_array(idf,describe=None,modify_col='RechargeConc',remove_fraction_array=None):
    '''Returns a modified dataframe with a reduced solute concentration where
    the reduction is a function of an array of reduction factors.'''
    
    idf['Before_' + describe] = idf[modify_col].copy()
    
    idf['FractionRemoved'] = idf.apply(lambda x:remove_fraction_array[x['Final Row']-1,x['Final Column']-1],axis=1)
    idf[modify_col] = idf[modify_col] * idf['FractionRemoved']
    idf = idf.drop('FractionRemoved',axis=1)

    return idf

def generate_ecdf(values):
    '''Generates travel time ecdf from vector of travel times.'''
    
    x = np.linspace(min(values),max(values),len(values))
    ecdf = sm.distributions.ECDF(values)
    y = ecdf(x)
    
    return x,y,np.mean(values),np.median(values)

def plot_ecdf(ixs,iys,imean,imedian):
    '''Plots the ECDF.'''

    fig,ax = plt.subplots(figsize=(4,3),dpi=300 )
    
    ax.semilogx(ixs,iys)
    ax.axvline(imean,label = 'Mean = %3.2f yrs' %(imean),ls='--')
    ax.axvline(imedian,label = 'Median = %3.2f yrs' %(imedian),ls=':')
    
    plt.xlabel('Baseflow Age (years)')
    plt.ylabel('Empirical\nCDF')
    plt.legend(loc='upper left',frameon=False)  
    plt.tight_layout()
    
    plt.show()
    
    return

def get_species(ilabel,species_list):
    '''Determines the species to which a transport observation belongs.'''

    ispecies = [x for x in species_list if x in ilabel][0]

    return ispecies
    
def summarize_sim_species(sim_df,obs_df):
    '''Returns a dictionary with key=observation location and value=travel time
    distribution. THIS IS WHERE STATISTICS FURTHER DESCRIBING THE
    TRAVEL TIME AND SOLUTE DISTRIBUTIONS SHOULD/COULD BE COMPUTED.'''
    
    species_summary_df = pd.DataFrame(columns=['ObsName','MeanTravel','StdTravel','MeanConcentration','StdConcentration'])
    icount = 0
    for iname,igrp in sim_df.groupby('ObsName'):
        
        # Weight the travel times and concentrations
        itravel  = igrp['ThisTravelTime']
        ispecies = igrp['RechargeConc']
        iweights = igrp['RechargeRate']
        
        # Compute statistics
        imean_travel = np.average(itravel,weights=iweights)
        ivariance_travel = np.average((itravel-imean_travel)**2,weights=iweights)
        istd_travel = math.sqrt(ivariance_travel)
        
        imean_species = np.average(ispecies,weights=iweights)
        ivariance_species = np.average((ispecies-imean_species)**2,weights=iweights)
        istd_species = math.sqrt(ivariance_species)
        
        species_summary_df.loc[icount,:] =[iname,imean_travel,istd_travel,imean_species,istd_species]
        icount += 1
        
    species_summary_df = pd.merge(species_summary_df,obs_df,on='ObsName')
        
    return species_summary_df


def write_sim_transport(idf,itob_order,itob_fout):
    '''Writes the simulated transport output to file.'''
    
    idf.columns = ['name','sim','obs']  # Rename for convenience
    idf['name'] = pd.Categorical(idf['name'],itob_order)
    idf = idf.sort_values(by='name')
    
    with open(itob_fout,'w') as fout:
        for idx,irow in idf.iterrows():            
            iname,isim,iobs = irow['name'],irow['sim'],irow['obs']
            fout.write('%-15s%15.6e%15.6e\n' %(iname,isim,iobs))
        
    return

# ---

class Endpoint(object):

    def __init__(self,endpoint_file=None,header_file=None,mpdirection=None,site_delim=None,term_status=None):
        '''The init generates the basic endpoint dataframe from the endpoint file.'''
        
        if mpdirection not in ['forward','backward']:
            quit('mpdirection kwarg must be "forward" or "backward".')
        
        self.direction = mpdirection
        
        header = open(header_file).readlines()
        header = [x.rstrip('\n') for x in header] # remove the \n from each item
        self.df_header = header         
        
        self.mp5_flag,self.header_line_count = read_header(endpoint_file)
        
        self.df = pd.read_table(endpoint_file,skiprows=self.header_line_count,header=None,delim_whitespace=True)
        self.df.columns = self.df_header
        
        if (self.mp5_flag == True): self.df = mp5_IPCODE(self.df)
        
        # Assume that negative travel times should = 0
        self.df['ThisTravelTime'] = self.df['Final Time'] - self.df['Initial Time']
        self.df.loc[self.df['ThisTravelTime']<0,'ThisTravelTime'] = 0
        
        # If site_delim (a delimiter used to distinguish multiple particles for
        # a given site) is provided, use it to associate a station name with
        # each particle
        if (site_delim is not None):
            self.df['station_nm'] = self.df['Label'].apply(lambda x:x.split(site_delim)[0])
            
        if (term_status is not None):
            self.df = self.df[self.df['Status'] == term_status]
        
        return

    def merge_tob_info(self,tob_csv_root,species_list):
        '''Reads .csv files containing solute observations and observation dates and merges
        with the endpoint dataframe.'''

        # Dataframe additions and conversions
        self.df['ObsName'] = self.df['Label'].apply(lambda x: x.split('-')[0])            # Strips particle count suffix 
        self.df['Species'] = self.df['Label'].apply(get_species,args=(species_list,))

        idf = pd.DataFrame()
        for isol in species_list:
            
            isol_df = pd.DataFrame.from_csv(tob_csv_root + isol + '.csv')[['ObsName','TobDate']]
            isol_df = pd.merge(isol_df,self.df,on='ObsName')
            idf = idf.append(isol_df)
            
        self.df = idf
        self.df['TobDate'] = pd.to_datetime(self.df['TobDate'])
        
        return
           
    def map_solute_input(self,input_ts_object=None,obs_df=None,downscale=None):
        '''Associates each particle with a recharging solute concentration. Note
        that this requires first associating a transport observation DATE with
        each particle. 'input_object' can be a dataframe (e.g., atm tracer inputs)
        or a dictionary (e.g., land surface nitrate inputs).'''
        
        obs_df['station_nm'] = obs_df['ObsName'].apply(lambda x:x.split('_')[0])
        
        # This loop generates a separate dataframe for each observation
        ispecies_df = pd.DataFrame()
        for idx,irow in obs_df.iterrows():

            # Get all the particles that originated at this observation location . . .
            isite = irow['station_nm']
            itob  = self.df[self.df['station_nm'] == isite].copy()
            
            # . . . and map them to the applicable locations for this
            # transport species
            itob['ObsName'] = idx
            itob['TobDate'] = pd.to_datetime(irow['TobDate'])
            
            itob['RechargeDate'] = itob.apply(get_recharge_date,axis=1,args=('backward',))
            itob['RechargeConc'] = itob.apply(value_from_date,axis=1,args=(input_ts_object,downscale))
            
            ispecies_df = ispecies_df.append(itob)

        return ispecies_df

    def add_discharge_dates(self,mp_start_date=None,quarterly=False):
        '''For forward-tracked particles: calculates the discharge date for
        terminating particles and adds temporal information to the dataframe.'''

        # Reduce the dataframe to terminated particles
        discharge_df = self.df.copy()        
        discharge_df = discharge_df[discharge_df['Status'] == terminated_status]
                        
        # Translate model times to calendar times and bin into months        
        discharge_df['DischargeDate'] = discharge_df['Final Time'].apply(lambda x: mp_start_date + timedelta(days=x))
        
        discharge_df['StartDate'] = discharge_df['Initial Time'].apply(lambda x: mp_start_date + timedelta(days=x))       
        discharge_df['DischargeMonth'] = discharge_df['DischargeDate'].apply(lambda x: int_to_month[x.month] + ' ' + str(x.year))
        
        if (quarterly ==  True):
            
            discharge_df['DischargeMonth'] = discharge_df['QuarterMonth']
            discharge_df.drop('QuarterMonth',axis=1,inplace=True)
            
        self.discharge_df = discharge_df
            
        return
        
    def get_discharge_zone_ecdfs(self,only_these_zones=None,ecdf_col='ThisTravelTime',times_mult=1./365.):
        '''Returns a dictionary with key=zone and values={'ECDF':(xs,ys),
        'Mean':mean travel time,'Median':median travel time}.'''
        
        if (only_these_zones is not None):
            self.discharge_df = self.discharge_df[self.discharge_df['Final Zone'].isin(only_these_zones)]
        
        ecdf_dict = {}
        for izone,igrp in self.discharge_df.groupby('Final Zone'):
            
            ixs,iys,imean,imedian = generate_ecdf(igrp[ecdf_col] * times_mult)
            ecdf_dict[izone] = {'ECDF':(ixs,iys),'Mean':imean,'Median':imedian}
        
        return ecdf_dict
        
    def get_age_ts(self):
        '''Returns a new dataframe that contains the age distribution
        characteristics for each time in the discharge time series.'''
        
        return