# -*- coding: utf-8 -*-
"""
Endpoint and pathline classes for post-processing MODPATH results.

Created on Tue Nov 10 08:57:21 2015
@author: wzell
"""
import math
import pandas as pd
import numpy as np
import datetime as dt
import statsmodels.api as sm

import matplotlib.pyplot as plt
plt.ioff()

# --- START MODULE PARAM SET ---

# See map_discharge_date_to_rch_date for info on this parameter
oldest_allowable_date = dt.datetime(1700,01,01) 

terminated_status = 2   # MP6 status flag for normally-terminated particles

int_to_month={1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}
month_to_int={'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}
quarter_map = {'Jan':'Jan','Feb':'Jan','Mar':'Jan','Apr':'Apr','May':'Apr','Jun':'Apr','Jul':'Jul','Aug':'Jul','Sep':'Jul','Oct':'Oct','Nov':'Oct','Dec':'Oct'}

recharge_to_volume = 1

# --- STOP MODULE PARAM SET ----
def value_from_date(irow,i_ts_object,mpdirection=None,check_col='RechargeInput_Lookup',\
                    isteady_input=True):
    """Finds the date index in a pandas SERIES that is NEAREST to the specified
    date and returns the associated value.i_ts_object can be a series
    (e.g., atm tracer inputs) or a dictionary of arrays (e.g., land surface
    nitrogen inputs).  Note that for nitrogen inputs this may associate a
    recharge date with the following year's estimated inputs.  This approximation
    deemed acceptable given the broader uncertainties associated with the loading
    estimations."""
    
    idate = irow[check_col]
    
    # The (row,col) index is a function of particle tracking direction.
    # Note the shift from MODFLOW to Python indexing for array selection.    
    if (mpdirection == 'backward'):
        jrow,jcol = irow['Final Row']-1,irow['Final Column']-1        
    if (mpdirection == 'forward'):
        jrow,jcol = irow['Initial Row']-1,irow['Initial Column']-1
   
    if (type(i_ts_object) is dict):
        # For case in which the passed time series argument is a dictionary:
        # Current version assumes that the check_col in the particle dataframe
        # maps to the keys in the input dictionary
    
        if (isteady_input == True):
            ival = i_ts_object[i_ts_object.keys()[0]][jrow,jcol]
            
        else:        
            if (idate < min(i_ts_object.keys())):
                ival = 0
            elif (idate > max(i_ts_object.keys())):
                ival = 0
            else:
                ival = i_ts_object[idate][jrow,jcol]
    
    else:
        # For the case in which the passed time series argument is a pandas Series.
        iseries = i_ts_object    
        idx = np.argmin(np.abs(iseries.index.to_pydatetime() - idate))
        ival = iseries.iloc[idx]

    return ival

def parse_IPCODE(row):
    '''Helper function for translating the MP5 IPCODE
    to MP6 status flags.'''
    
    ip_code = row.IPCODE
    
    if (ip_code < 0):
        id_code = str(int(ip_code))
    else:
        id_code = list(str(int(ip_code)))[-1]

    if (id_code == '-2'): # Unreleased
        istatus = 4
    elif (id_code == '-1'): # Stranded
        istatus = 5
    elif (id_code == '0'): # Still active
        istatus = 1  
    elif (id_code == '1'): # Normally terminated
        istatus = 2
    elif (id_code == '2'): # Zone terminated
        istatus = 3  
    
    return istatus

def mp5_IPCODE(idf):
    '''Determines particle status from the IPCODE. NOTE: uses MP6 status
    convention rather than MP5 IDCODE convention! MP5 IDCODE 1 = MP6 status
    code 2 = discharged normally.'''
        
    idf['Status'] = idf.apply(parse_IPCODE,axis=1)
    idf['Particle ID'] = idf.index
    
    return idf

def read_header(endpoint_file):
    '''Reads the endpoint header, determines whether the endpoint file was
    generated by MODPATH5 or MODPATH6, and counts the number of header lines.'''
    
    with open(endpoint_file,'r') as fin:
        
        all_lines = fin.readlines()
        if 'MODPATH_ENDPOINT_FILE 6' in all_lines[0]:                
            mp5_flag = False
            header_line_count = 1
            for iline in all_lines:
                if 'END HEADER' in iline:
                    break
                else:
                    header_line_count += 1            
            fin.close()
            
        else:
            mp5_flag = True
            header_line_count = 1
            
    return mp5_flag,header_line_count

def parse_endpoint(endpoint_file):
    '''Split the MODPATH endpoint file into two files; normally terminated and still active.
    This splitting may be required in order to accomodate the memory limits encountered when
    reading very large files to a dataframe.'''

    print '\n\nSplitting endpoint file into active and terminated subsets.\n\n'
    
    # Initialize the function parameters   
    active_endpoint_file = endpoint_file + '.active','w'
    terminated_endpoint_file = endpoint_file + '.terminated','w'   
    
    mp5_flag,header_line_count = read_header(endpoint_file)
    
    if (mp5_flag == True):
        status_idx = 17
    else:
        status_idx = 2
        
    with open(endpoint_file,'r') as fin,\
         open(active_endpoint_file) as active_fout,\
         open(terminated_endpoint_file) as terminated_fout:
        
        icount = 0
        for iline in fin:
    
            # Write the header lines           
            while (icount < header_line_count):
                
                active_fout.write(iline)
                terminated_fout.write(iline)
                    
                icount += 1
                continue
            
            istatus = parse_IPCODE(iline.split()[status_idx])            
            
            if (istatus == 1):
                active_fout.write(iline)
                
            if (istatus == 2):
                terminated_fout.write(iline)
                
    return

def write_restart(self,active_df,terminated_df,restart_csv,restart_flag):
    '''Merge the information on restarting particles (derived from a 
    previous endpoint file) with the information on those same particles
    from this endpoint file. NOTE: this assumes that the endpoint file has
    been parsed into terminated and active components.  With modification this
    could also accomodate a full endpoint dataframe that has not yet been split.
    E.g., - 'if terminated_df is not None' . . . and if terminated_df is None,
    assume that the endpoint file has not been split and handle the sorting here.'''

    # Housekeeping for the active_df
    active_df = active_df[['Initial Time','Initial Global X','Initial Global Y','Initial Local Z','Final Column','Final Row','Final Layer','Final Global X','Final Global Y','Final Local Z','ThisTravelTime']]
    
    # If restarting, identify those active particles which were in the system before this model run . . . 
    if (restart_flag == True):
        
        # Load the information from the previous model run        
        restart_df = pd.DataFrame.from_csv(restart_csv)
        restart_df = restart_df.drop(['ThisTravelTime'],axis=1)
                        
        # Read the local coords as strings in order to fix them for use as identifiers
        # in endpoint processing
        for icol in ['Initial Global X','Initial Global Y','Initial Local Z']:
            restart_df[icol] = restart_df[icol].astype(str)
            active_df[icol] = active_df[icol].astype(str)
            terminated_df[icol] = terminated_df[icol].astype(str)

        # Merge the active particles with the restart df in order to associate any prior travel time with each active particle;
        # increment the cumulative time to include all travel time
        active_df = pd.merge(active_df,restart_df,on=(['Initial Global X','Initial Global Y','Initial Local Z']),how='left')
        active_df['CumTravelTime'] = active_df['CumTravelTime'].fillna(0)
        active_df['CumTravelTime'] = active_df['CumTravelTime'] + active_df['ThisTravelTime']
        active_df['Initial Time'] = active_df['Initial Time'] - active_df['CumTravelTime']

        # The modified active_df will be saved as the restart dataframe
        new_restart_df = active_df[['Final Column','Final Row','Final Layer','Final Global X','Final Global Y','Final Local Z','ThisTravelTime','CumTravelTime']]
               
        # Merge the terminated particles with the restart_df
        terminated_df = pd.merge(terminated_df,restart_df,on=['Initial Row','Initial Column','Initial Layer','Initial Global X','Initial Global Y','Initial Local Z'],how='left')
        terminated_df['CumTravelTime'] = terminated_df['CumTravelTime'].fillna(0)
        terminated_df['Initial Time'] = terminated_df['Initial Time'] - terminated_df['CumTravelTime']

    # If not restarting (restart_flag = False), the unmodified active_df will be saved as the restart dataframe    
    else:
        new_restart_df = active_df[['Final Column','Final Row','Final Layer','Final Global X','Final Global Y','Final Local Z','ThisTravelTime']]
        new_restart_df['CumTravelTime'] = new_restart_df['ThisTravelTime']
        
    # Save the new restart df with appropriately re-labeled columns    
    new_restart_df.columns = [x.replace('Final','Initial') for x in new_restart_df.columns]
    new_restart_df.to_csv(restart_csv)
    
    return terminated_df

# ================================
# --- START RECHARGE FUNCTIONS ---
# ================================

def get_rch_date(row,idirection,discharge_dates=None):
    '''HELPER FUNCTION that should not be called directly by the user.
    For transport observations simulated with backward tracking,
    returns the recharge date given the observation date and the travel time
    to that observation. NO FORWARD METHOD IMPLEMENTED (see map_discharge_dates()).'''
          
    if (idirection == 'backward'):
        return (row['TobDate'] - timedelta(days=(row['ThisTravelTime'])))
        
    if (idirection == 'forward'):
        return None

def get_rch_volume_per_particle(row,iarea,time_length=365.25):
    '''Returns the recharge volume assigned to each cell. NEEDS UPDATE 
    (including supporting work elsewhere) in order to derive actual time step
    length). Current version assumes steady state, with annual time step
    for solute input time series (and thus an interest in getting volume
    recharge/year).''' 
    
    irate = row['RechargeRate']
    iparticles = row['NParticles']    
    ivol = (irate * time_length * iarea)/iparticles
    
    return ivol

def map_rch_input(idf,rch_ts_dict,nx=1,ny=1,nz=1,cell_area=None,mpdirection=None,ss_input=True):
    '''Associates each particle in a dataframe with a recharge rate (NEED TO CHECK RECHARGE VOLUME).
    Currently assumes steady state simulation; NEEDS METHOD OF ASSOCIATING PARTICLE START TIME WITH
    SELECTION FROM RECHARGE TIME SERIES. Consider using value_from_date function.
    rch_ts_dict = recharge time series dictionary [keys=time
    step,values=(nrow,ncol).'''
    
    if (mpdirection == 'backward'):
        # For backward-particle simulations: calculates the recharge date for
        # terminating particles and adds temporal information to the dataframe.'''
        idf['RechargeRate'] = idf.apply(get_recharge_rate,axis=1,args=(rch_ts_dict,'backward'))
        idf['RechargeVol']  = idf['RechargeRate'] * recharge_to_volume                          # <- NOT CORRECT # # #
        
    if (mpdirection == 'forward'):       
        
        # For forward-particle simulations: extract the recharge rate for
        # starting particles from the recharge time series
        idf['NParticles'] = nx * ny * nz
        idf['RechargeRate'] = idf.apply(value_from_date,axis=1,args=(rch_ts_dict,'forward'))
        idf['RechargeVol']  = idf.apply(get_rch_volume_per_particle,axis=1,args=(cell_area,))
                        
    return idf

# ================================
# --- STOP RECHARGE FUNCTIONS ----
# ================================

def modify_all(idf,describe=None,modify_col=None,remove_fraction=None):
    '''Returns a modified dataframe with a reduced solute concentration
    for all particles.  E.g., for simulation of nitrate removal by stream
    removal, in which the input dataframe has already been filtered by
    zone such that all particles in the dataframe belong to the same zone
    and as a result are subject to the same removal fraction.'''
    
    idf['Before_' + describe] = idf[modify_col].copy()
    idf[modify_col] = idf[modify_col] * remove_fraction
    
    return idf
    
def modify_by_list(idf,describe=None,filter_by_col=None,filter_list=None,\
                   modify_col=None,remove_fraction=1):
    '''Returns a modified dataframe with a reduced
    solute concentration if the particle is flagged by the remove_by_col argument.
    E.g., for simulation of nitrate removal by denitrification, a list of particle
    ids of those particles which contact the Aquia Confining Unit may be passed,
    together with a removal efficiency of the Confining Unit, in order to
    reduce the total simulated nitrate concentration at the observation
    location.'''    
    
    idf['Before_' + describe] = idf[modify_col].copy()

    # Create the subset of particles which may be removed
    idf['FractionRemaining'] = 1       # Initializes all columns with no removal
    idf.loc[idf[filter_by_col].isin(filter_list),'FractionRemaining'] = 1 - remove_fraction
    idf[modify_col] = idf[modify_col] * idf['FractionRemaining']
    idf = idf.drop('FractionRemaining',axis=1)
    
    return idf
    
def modify_by_array(idf,describe=None,modify_col=None,remove_fraction_array=None,mpdirection=None):
    '''Returns a modified dataframe with a reduced solute concentration where
    the reduction is a function of an array of reduction factors.'''
    
    # The (row,col) index is a function of particle tracking direction.
    # Note the shift from MODFLOW to Python indexing for array selection.    
    if (mpdirection == 'backward'):
        irow_colname,icol_colname = 'Final Row','Final Column'        
    if (mpdirection == 'forward'):
        irow_colname,icol_colname = 'Initial Row','Initial Column'
    
    idf['Before_' + describe] = idf[modify_col].copy()
    
    idf['FractionRemoved'] = idf.apply(lambda x:remove_fraction_array[x[irow_colname]-1,x[icol_colname]-1],axis=1)
    idf[modify_col] = idf[modify_col] * idf['FractionRemoved']
    idf = idf.drop('FractionRemoved',axis=1)

    return idf

def generate_ecdf(values):
    '''Generates travel time ecdf from vector of travel times.'''
    
    x = np.linspace(min(values),max(values),len(values))
    ecdf = sm.distributions.ECDF(values)
    y = ecdf(x)
    
    return x,y,np.mean(values),np.median(values)

def plot_ecdf(ixs,iys,imean,imedian):
    '''Plots the ECDF.'''

    fig,ax = plt.subplots(figsize=(4,3),dpi=300 )
    
    ax.semilogx(ixs,iys)
    ax.axvline(imean,label = 'Mean = %3.2f yrs' %(imean),ls='--')
    ax.axvline(imedian,label = 'Median = %3.2f yrs' %(imedian),ls=':')
    
    plt.xlabel('Baseflow Age (years)')
    plt.ylabel('Empirical\nCDF')
    plt.legend(loc='upper left',frameon=False)  
    plt.tight_layout()
    
    plt.show()
    
    return

def get_species(ilabel,species_list):
    '''Determines the species to which a transport observation belongs.'''

    ispecies = [x for x in species_list if x in ilabel][0]

    return ispecies
    
def summarize_sim_species(sim_df,obs_df):
    '''Returns a dictionary with key=observation location and value=travel time
    distribution. THIS IS WHERE STATISTICS FURTHER DESCRIBING THE
    TRAVEL TIME AND SOLUTE DISTRIBUTIONS SHOULD/COULD BE COMPUTED.'''
    
    species_summary_df = pd.DataFrame(columns=['ObsName','MeanTravel','StdTravel','MeanConcentration','StdConcentration'])
    icount = 0
    for iname,igrp in sim_df.groupby('ObsName'):
        
        # Weight the travel times and concentrations
        itravel  = igrp['ThisTravelTime']
        ispecies = igrp['RechargeConc']
        iweights = igrp['RechargeRate']
        
        # Compute statistics
        imean_travel = np.average(itravel,weights=iweights)
        ivariance_travel = np.average((itravel-imean_travel)**2,weights=iweights)
        istd_travel = math.sqrt(ivariance_travel)
        
        imean_species = np.average(ispecies,weights=iweights)
        ivariance_species = np.average((ispecies-imean_species)**2,weights=iweights)
        istd_species = math.sqrt(ivariance_species)
        
        species_summary_df.loc[icount,:] =[iname,imean_travel,istd_travel,imean_species,istd_species]
        icount += 1
        
    species_summary_df = pd.merge(species_summary_df,obs_df,on='ObsName')
        
    return species_summary_df

def write_sim_transport(idf,itob_order,itob_fout):
    '''Writes the simulated transport output to file.'''
    
    idf.columns = ['name','sim','obs']  # Rename for convenience
    idf['name'] = pd.Categorical(idf['name'],itob_order)
    idf = idf.sort_values(by='name')
    
    with open(itob_fout,'w') as fout:
        for idx,irow in idf.iterrows():            
            iname,isim,iobs = irow['name'],irow['sim'],irow['obs']
            fout.write('%-15s%15.6e%15.6e\n' %(iname,isim,iobs))
        
    return

def map_solute_input(idf,input_ts_object=None,obs_df=None,mpdirection=None):
    '''Associates each particle in the input dataframe with a
    recharging solute concentration. Note that for backtracked particles
    this requires first associating a transport observation DATE with each particle.
    For forward tracked particles the input dataframe must already include the
    recharge date. 'input_ts_object' can be a
    dataframe (e.g., atm tracer inputs) or a dictionary (e.g., land surface nitrate inputs).'''

    if any(substring in mpdirection for substring in ['FOR','For','for']):
        
        idf['RechargeConc'] = idf.apply(value_from_date,axis=1,args=(input_ts_object,'forward'),isteady_input=False)

        return idf

    if any(substring in mpdirection for substring in ['BACK','Back','back']):
        
        obs_df['station_nm'] = obs_df['ObsName'].apply(lambda x:x.split('_')[0])
        
        # This loop generates a separate dataframe for each observation
        ispecies_df = pd.DataFrame()
        for idx,irow in obs_df.iterrows():
    
            # Get all the particles that originated at this observation location . . .
            isite = irow['station_nm']
            itob  = idf[idf['station_nm'] == isite].copy()
            
            # . . . and map them to the applicable locations for this
            # transport species
            itob['ObsName'] = idx
            itob['TobDate'] = pd.to_datetime(irow['TobDate'])
            
            itob['RechargeDate'] = itob.apply(get_rch_date,axis=1,args=('backward',))
            itob['RechargeConc'] = itob.apply(value_from_date,axis=1,args=(input_ts_object,downscale,'backward'))
            
            ispecies_df = ispecies_df.append(itob)
    
        return ispecies_df

# ---

class Endpoint(object):

    def __init__(self,endpoint_file=None,header_file=None,mpdirection=None,site_delim=None,term_status=None):
        '''The init generates the basic endpoint dataframe from the endpoint file.'''
        
        if mpdirection not in ['forward','backward']:
            quit('mpdirection kwarg must be "forward" or "backward".')
        
        self.direction = mpdirection
        
        header = open(header_file).readlines()
        header = [x.rstrip('\n') for x in header] # remove the \n from each item
        self.df_header = header         
        
        self.mp5_flag,self.header_line_count = read_header(endpoint_file)
        
        self.df = pd.read_table(endpoint_file,skiprows=self.header_line_count,header=None,delim_whitespace=True)
        self.df.columns = self.df_header
        
        if (self.mp5_flag == True): self.df = mp5_IPCODE(self.df)
        
        # Assume that negative travel times should = 0
        self.df['ThisTravelTime'] = self.df['Final Time'] - self.df['Initial Time']
        self.df.loc[self.df['ThisTravelTime']<0,'ThisTravelTime'] = 0
        
        # If site_delim (a delimiter used to distinguish multiple particles for
        # a given site) is provided, use it to associate a station name with
        # each particle
        if (site_delim is not None):
            self.df['station_nm'] = self.df['Label'].apply(lambda x:x.split(site_delim)[0])
            
        if (term_status is not None):
            self.df = self.df[self.df['Status'] == term_status]
        
        return

    def merge_tob_info(self,tob_csv_root,species_list):
        '''Reads .csv files containing solute observations and observation dates and merges
        with the endpoint dataframe.'''

        # Dataframe additions and conversions
        self.df['ObsName'] = self.df['Label'].apply(lambda x: x.split('-')[0])            # Strips particle count suffix 
        self.df['Species'] = self.df['Label'].apply(get_species,args=(species_list,))

        idf = pd.DataFrame()
        for isol in species_list:
            
            isol_df = pd.DataFrame.from_csv(tob_csv_root + isol + '.csv')[['ObsName','TobDate']]
            isol_df = pd.merge(isol_df,self.df,on='ObsName')
            idf = idf.append(isol_df)
            
        self.df = idf
        self.df['TobDate'] = pd.to_datetime(self.df['TobDate'])
        
        return

    def map_discharge_date_to_rch_date(self,discharge_dates=None,track_discharge_zones=None):
        '''For forward tracking, returns a dictionary of dictionaries,
        {discharge zone id: {discharge date: endpoint dataframe w/ recharge date}}
        'discharge_dates' is a pandas time series.
        Note that the datetime and timedelta processes collapse (resulting in
        an overflow error) with dates earlier than the mid 17th century
        It may be necessary at some point in the
        future to get precise dates for these instances; for now, however, particles
        that encounter this constraint are simply assigned a minimum recharge
        date as specified in the script parameters (above).'''
        
        discharge_df = self.df.copy()
        
        # Replace problematic old dates with the longest admissable travel time
        traveltime_cutoff = (discharge_dates[0] - oldest_allowable_date).days
        discharge_df['ThisTravelTime'].loc[discharge_df['ThisTravelTime'] > traveltime_cutoff] = traveltime_cutoff
        
        if track_discharge_zones is None:        
            track_discharge_zones = discharge_df['Final Zone'].unique()
       
        # For subsequent operations involving vectorized timedelta operations,
        # convert the discharge_dates and the travel times to numpy arrays
        discharge_dates_vector = np.array(discharge_dates)
        
        # Generate the dictionary of dataframes
        discharge_dict = {}
        
        # Subset the zones . . .
        for izone in track_discharge_zones:
            
            print 'Calculating recharge dates for all discharge dates in zone %i' %(izone)
            
            discharge_dict[izone] = {}
            
            idf = discharge_df[discharge_df['Final Zone'] == izone]
            itraveltimes_vector = np.array(idf['ThisTravelTime'],dtype='timedelta64[D]')
            
            # . . . then iterate on the dates
            for idate in discharge_dates_vector:
                
                # Revert the date to a datetime object for use as the dict key
                idate_key = pd.to_datetime(idate)
                
                # Create a vector of constants where constant = discharge_date[i]
                idischarge_date_vector = np.array([idate for x in range(len(itraveltimes_vector))])
                
                # Subtract the particle travel time to get the recharge date    
                irch_date_vector = idischarge_date_vector - itraveltimes_vector
            
                # Build the dataframe for this discharge date
                # In order to speed up the mapping of solute inputs to recharge
                # date, make a simplified (i.e., extract the year) recharge date.
                # The solute input time series will require the same date simplification
                # as keys in the solute input dictionary.
                idf['RechargeDate'] = irch_date_vector
                idf['RechargeInput_Lookup'] = pd.DatetimeIndex(idf['RechargeDate']).year
                discharge_dict[izone][idate_key] = idf
                                                               
        return discharge_dict
 
    def map_discharge_dates(self,mp_start_date=None,quarterly=False):
        '''For forward-tracked particles: calculates the discharge date for
        terminating particles and adds temporal information to the dataframe.'''

        # Reduce the dataframe to terminated particles
        discharge_df = self.df.copy()        
        discharge_df = discharge_df[discharge_df['Status'] == terminated_status]
                        
        # Translate model times to calendar times and bin into months        
        discharge_df['DischargeDate'] = discharge_df['Final Time'].apply(lambda x: mp_start_date + timedelta(days=x))
        
        discharge_df['StartDate'] = discharge_df['Initial Time'].apply(lambda x: mp_start_date + timedelta(days=x))       
        discharge_df['DischargeMonth'] = discharge_df['DischargeDate'].apply(lambda x: int_to_month[x.month] + ' ' + str(x.year))
        
        if (quarterly ==  True):
            
            discharge_df['DischargeMonth'] = discharge_df['QuarterMonth']
            discharge_df.drop('QuarterMonth',axis=1,inplace=True)
            
        self.discharge_df = discharge_df
            
        return
        
    def get_discharge_zone_ecdfs(self,only_these_zones=None,ecdf_col='ThisTravelTime',times_mult=1./365.):
        '''Returns a dictionary with key=zone and values={'ECDF':(xs,ys),
        'Mean':mean travel time,'Median':median travel time}.'''
        
        if (only_these_zones is not None):
            self.discharge_df = self.discharge_df[self.discharge_df['Final Zone'].isin(only_these_zones)]
        
        ecdf_dict = {}
        for izone,igrp in self.discharge_df.groupby('Final Zone'):
            
            ixs,iys,imean,imedian = generate_ecdf(igrp[ecdf_col] * times_mult)
            ecdf_dict[izone] = {'ECDF':(ixs,iys),'Mean':imean,'Median':imedian}
        
        return ecdf_dict
        
    def get_age_ts(self):
        '''Returns a new dataframe that contains the age distribution
        characteristics for each time in the discharge time series.'''
        
        return